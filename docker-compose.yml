services:
  # Tika service for PDF support
  tika:
    build:
      context: .
      dockerfile: Dockerfile.tika
    image: jic-tika:cached
    container_name: tika
    ports:
      - "9998:9998"

  # C++ server
  jic:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        BUILDKIT_PROGRESS: plain
        DOCKER_BUILDKIT: 1
    container_name: jic
    depends_on:
      - tika
    deploy:
      resources:
        limits:
          cpus: '0'  # No CPU limit - use all available cores
          memory: 8G
    volumes:
      - ./public:/app/public
      - ./data:/app/data
      - ./gguf_models:/app/gguf_models:ro
    ports:
      - "8080:8080"
    environment:
      - TIKA_URL=http://tika:9998
      # Model Configuration - Change here to switch models
      - LLM_MODEL=qwen2.5-vl:7b
      - EMBEDDING_MODEL=nomic-embed-text
      - LLM_GGUF_FILE=Qwen2.5-VL-7B-Instruct-Q4_K_M.gguf
      - LLM_MMPROJ_FILE=mmproj-Qwen2.5-VL-7B-Instruct-f16.gguf
      - EMBEDDING_GGUF_FILE=nomic-embed-text-v1.5.Q4_K_M.gguf
    command: ["./jic-server"]

  # Ingestion service
  ingestion:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        BUILDKIT_PROGRESS: plain
        DOCKER_BUILDKIT: 1
    container_name: ingestion
    depends_on:
      - tika
    deploy:
      resources:
        limits:
          cpus: '2'  # Limit CPU usage for ingestion
          memory: 4G
    volumes:
      - ./public:/app/public
      - ./data:/app/data
      - ./gguf_models:/app/gguf_models:ro
    environment:
      - TIKA_URL=http://tika:9998
      # Model Configuration - Change here to switch models
      - LLM_MODEL=qwen2.5-vl:7b
      - EMBEDDING_MODEL=nomic-embed-text
      - LLM_GGUF_FILE=Qwen2.5-VL-7B-Instruct-Q4_K_M.gguf
      - LLM_MMPROJ_FILE=mmproj-Qwen2.5-VL-7B-Instruct-f16.gguf
      - EMBEDDING_GGUF_FILE=nomic-embed-text-v1.5.Q4_K_M.gguf
    command: ["./jic-ingestion"]

